{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOiD6ZJPYBL5MexKW/o/FFV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":402},"id":"ym5_5SJaSYN2","executionInfo":{"status":"error","timestamp":1741035010484,"user_tz":300,"elapsed":460,"user":{"displayName":"Nolan Knight","userId":"18401773510958510497"}},"outputId":"b43f72b5-7ce7-4c6a-9287-b52a6f0c12fa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'common'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-bac18f0492ce>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m from common import (\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mDEFAULT_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mDEFAULT_ITERATIONS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'common'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["#\n","# For licensing see accompanying LICENSE file.\n","# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n","#\n","\n","import argparse\n","import math\n","from typing import List, Optional\n","\n","import torch\n","from torch.cuda.amp import GradScaler\n","from torch.distributed.elastic.multiprocessing import errors\n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","folder_path = '/content/drive/MyDrive/ColabNotebooks/GhostNet'  # Replace with your folder path\n","sys.path.append(folder_path)\n","\n","from common import (\n","    DEFAULT_EPOCHS,\n","    DEFAULT_ITERATIONS,\n","    DEFAULT_MAX_EPOCHS,\n","    DEFAULT_MAX_ITERATIONS,\n",")\n","from cvnets import EMA, get_model\n","from data import create_train_val_loader\n","from engine import Trainer\n","from loss_fn import build_loss_fn\n","from optim import build_optimizer\n","from optim.scheduler import build_scheduler\n","from options.opts import get_training_arguments\n","from utils import logger, resources\n","from utils.checkpoint_utils import load_checkpoint, load_model_state\n","from utils.common_utils import create_directories, device_setup\n","from utils.ddp_utils import distributed_init, is_master\n","\n","\n","@errors.record\n","def main(opts: argparse.Namespace, **kwargs) -> None:\n","    # defaults are for CPU\n","    dev_id = getattr(opts, \"dev.device_id\", torch.device(\"cpu\"))\n","    device = getattr(opts, \"dev.device\", torch.device(\"cpu\"))\n","    use_distributed = getattr(opts, \"ddp.use_distributed\")\n","\n","    is_master_node = is_master(opts)\n","\n","    # set-up data loaders\n","    train_loader, val_loader, train_sampler = create_train_val_loader(opts)\n","\n","    # compute max iterations based on max epochs\n","    # Useful in doing polynomial decay\n","    is_iteration_based = getattr(opts, \"scheduler.is_iteration_based\")\n","    if is_iteration_based:\n","        max_iter = getattr(opts, \"scheduler.max_iterations\", DEFAULT_ITERATIONS)\n","        if max_iter is None or max_iter <= 0:\n","            logger.log(\"Setting max. iterations to {}\".format(DEFAULT_ITERATIONS))\n","            setattr(opts, \"scheduler.max_iterations\", DEFAULT_ITERATIONS)\n","            max_iter = DEFAULT_ITERATIONS\n","        setattr(opts, \"scheduler.max_epochs\", DEFAULT_MAX_EPOCHS)\n","        if is_master_node:\n","            logger.log(\"Max. iteration for training: {}\".format(max_iter))\n","    else:\n","        max_epochs = getattr(opts, \"scheduler.max_epochs\", DEFAULT_EPOCHS)\n","        if max_epochs is None or max_epochs <= 0:\n","            logger.log(\"Setting max. epochs to {}\".format(DEFAULT_EPOCHS))\n","            setattr(opts, \"scheduler.max_epochs\", DEFAULT_EPOCHS)\n","        setattr(opts, \"scheduler.max_iterations\", DEFAULT_MAX_ITERATIONS)\n","        max_epochs = getattr(opts, \"scheduler.max_epochs\", DEFAULT_EPOCHS)\n","        if is_master_node:\n","            logger.log(\"Max. epochs for training: {}\".format(max_epochs))\n","    # set-up the model\n","    model = get_model(opts)\n","    # print model information on master node\n","    if is_master_node:\n","        model.info()\n","\n","    # memory format\n","    memory_format = (\n","        torch.channels_last\n","        if getattr(opts, \"common.channels_last\")\n","        else torch.contiguous_format\n","    )\n","\n","    model = model.to(device=device, memory_format=memory_format)\n","\n","    if getattr(opts, \"ddp.use_deprecated_data_parallel\"):\n","        logger.warning(\n","            \"DataParallel is not recommended for training, and is not tested exhaustively. \\\n","                Please use it only for debugging purposes. We will deprecated the support for DataParallel in future and \\\n","                    encourage you to use DistributedDataParallel.\"\n","        )\n","        model = model.to(memory_format=memory_format, device=torch.device(\"cpu\"))\n","        model = torch.nn.DataParallel(model)\n","        model = model.to(device=device)\n","    elif use_distributed:\n","        model = torch.nn.parallel.DistributedDataParallel(\n","            model,\n","            device_ids=[dev_id],\n","            output_device=dev_id,\n","            find_unused_parameters=getattr(opts, \"ddp.find_unused_params\"),\n","        )\n","        if is_master_node:\n","            logger.log(\"Using DistributedDataParallel for training\")\n","\n","    # create loss function, print its information, and move to device\n","    criteria = build_loss_fn(opts)\n","    if is_master_node:\n","        logger.log(logger.color_text(\"Loss function\"))\n","        print(criteria)\n","    criteria = criteria.to(device=device)\n","\n","    # create the optimizer and print its information\n","    optimizer = build_optimizer(model, opts=opts)\n","    if is_master_node:\n","        logger.log(logger.color_text(\"Optimizer\"))\n","        print(optimizer)\n","\n","    # create the gradient scalar\n","    gradient_scaler = GradScaler(enabled=getattr(opts, \"common.mixed_precision\"))\n","\n","    # LR scheduler\n","    scheduler = build_scheduler(opts=opts)\n","    if is_master_node:\n","        logger.log(logger.color_text(\"Learning rate scheduler\"))\n","        print(scheduler)\n","\n","    model_ema = None\n","    use_ema = getattr(opts, \"ema.enable\")\n","\n","    if use_ema:\n","        ema_momentum = getattr(opts, \"ema.momentum\")\n","        model_ema = EMA(model=model, ema_momentum=ema_momentum, device=device)\n","        if is_master_node:\n","            logger.log(\"Using EMA\")\n","\n","    best_metric = 0.0 if getattr(opts, \"stats.checkpoint_metric_max\") else math.inf\n","\n","    start_epoch = 0\n","    start_iteration = 0\n","    resume_loc = getattr(opts, \"common.resume\")\n","    finetune_loc = getattr(opts, \"common.finetune\")\n","    auto_resume = getattr(opts, \"common.auto_resume\")\n","    if resume_loc is not None or auto_resume:\n","        (\n","            model,\n","            optimizer,\n","            gradient_scaler,\n","            start_epoch,\n","            start_iteration,\n","            best_metric,\n","            model_ema,\n","        ) = load_checkpoint(\n","            opts=opts,\n","            model=model,\n","            optimizer=optimizer,\n","            model_ema=model_ema,\n","            gradient_scaler=gradient_scaler,\n","        )\n","    elif finetune_loc is not None:\n","        model, model_ema = load_model_state(opts=opts, model=model, model_ema=model_ema)\n","        if is_master_node:\n","            logger.log(\"Finetuning model from checkpoint {}\".format(finetune_loc))\n","\n","    training_engine = Trainer(\n","        opts=opts,\n","        model=model,\n","        validation_loader=val_loader,\n","        training_loader=train_loader,\n","        optimizer=optimizer,\n","        criterion=criteria,\n","        scheduler=scheduler,\n","        start_epoch=start_epoch,\n","        start_iteration=start_iteration,\n","        best_metric=best_metric,\n","        model_ema=model_ema,\n","        gradient_scaler=gradient_scaler,\n","    )\n","\n","    training_engine.run(train_sampler=train_sampler)\n","\n","\n","def distributed_worker(i, main, opts, kwargs):\n","    setattr(opts, \"dev.device_id\", i)\n","    torch.cuda.set_device(i)\n","    setattr(opts, \"dev.device\", torch.device(f\"cuda:{i}\"))\n","\n","    ddp_rank = getattr(opts, \"ddp.rank\", None)\n","    if ddp_rank is None:  # torch.multiprocessing.spawn\n","        ddp_rank = kwargs.get(\"start_rank\", 0) + i\n","        setattr(opts, \"ddp.rank\", ddp_rank)\n","\n","    node_rank = distributed_init(opts)\n","    setattr(opts, \"ddp.rank\", node_rank)\n","    main(opts, **kwargs)\n","\n","\n","def main_worker(args: Optional[List[str]] = None, **kwargs):\n","    opts = get_training_arguments(args=args)\n","    print(opts)\n","    # device set-up\n","    opts = device_setup(opts)\n","\n","    node_rank = getattr(opts, \"ddp.rank\")\n","    if node_rank < 0:\n","        logger.error(\"--rank should be >=0. Got {}\".format(node_rank))\n","\n","    is_master_node = is_master(opts)\n","\n","    # create the directory for saving results\n","    save_dir = getattr(opts, \"common.results_loc\")\n","    run_label = getattr(opts, \"common.run_label\")\n","    exp_dir = \"{}/{}\".format(save_dir, run_label)\n","    setattr(opts, \"common.exp_loc\", exp_dir)\n","    create_directories(dir_path=exp_dir, is_master_node=is_master_node)\n","\n","    num_gpus = getattr(opts, \"dev.num_gpus\")\n","    world_size = getattr(opts, \"ddp.world_size\")\n","\n","    # use DDP if num_gpus is > 1\n","    use_distributed = True if num_gpus > 1 else False\n","    setattr(opts, \"ddp.use_distributed\", use_distributed)\n","\n","    if num_gpus > 0:\n","        assert torch.cuda.is_available(), \"We need CUDA for training on GPUs.\"\n","\n","    # No of data workers = no of CPUs (if not specified or -1)\n","    n_cpus = resources.cpu_count()\n","    dataset_workers = getattr(opts, \"dataset.workers\", -1)\n","\n","    if getattr(opts, \"ddp.use_deprecated_data_parallel\") or num_gpus <= 1:\n","        if dataset_workers == -1:\n","            setattr(opts, \"dataset.workers\", n_cpus)\n","\n","        # adjust the batch size\n","        train_bsize = getattr(opts, \"dataset.train_batch_size0\") * max(1, num_gpus)\n","        val_bsize = getattr(opts, \"dataset.val_batch_size0\") * max(1, num_gpus)\n","        setattr(opts, \"dataset.train_batch_size0\", train_bsize)\n","        setattr(opts, \"dataset.val_batch_size0\", val_bsize)\n","        setattr(opts, \"dev.device_id\", None)\n","        main(opts=opts, **kwargs)\n","\n","    else:\n","        # DDP is the default for training\n","\n","        # get device id\n","        dev_id = getattr(opts, \"ddp.device_id\")\n","        # set the dev.device_id to the same as ddp.device_id.\n","        # note that dev arguments are not accessible through CLI.\n","        setattr(opts, \"dev.device_id\", dev_id)\n","\n","        if world_size == -1:\n","            logger.log(\n","                \"Setting --ddp.world-size the same as the number of available gpus\"\n","            )\n","            world_size = num_gpus\n","            setattr(opts, \"ddp.world_size\", world_size)\n","\n","        if dataset_workers == -1 or dataset_workers is None:\n","            setattr(opts, \"dataset.workers\", n_cpus // num_gpus)\n","\n","        start_rank = getattr(opts, \"ddp.rank\")\n","        # we need to set rank to None as we set it inside the distributed_worker\n","        setattr(opts, \"ddp.rank\", None)\n","        kwargs[\"start_rank\"] = start_rank\n","        setattr(opts, \"ddp.start_rank\", start_rank)\n","        torch.multiprocessing.spawn(\n","            fn=distributed_worker,\n","            args=(main, opts, kwargs),\n","            nprocs=num_gpus,\n","        )\n","\n","\n","if __name__ == \"__main__\":\n","    main_worker()"]}]}