{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNB1RfLd5dA75Mhzjkt2Vq+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"EBJD7t3Xcy1P"},"outputs":[],"source":["# 2020.06.09-Changed for building GhostNet\n","#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n","\"\"\"\n","Creates a GhostNet Model as defined in:\n","GhostNet: More Features from Cheap Operations By Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, Chang Xu.\n","https://arxiv.org/abs/1911.11907\n","Modified from https://github.com/d-li14/mobilenetv3.pytorch and https://github.com/rwightman/pytorch-image-models\n","\"\"\"\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","\n","\n","__all__ = ['ghost_net']\n","\n","\n","def _make_divisible(v, divisor, min_value=None):\n","    \"\"\"\n","    This function is taken from the original tf repo.\n","    It ensures that all layers have a channel number that is divisible by 8\n","    It can be seen here:\n","    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n","    \"\"\"\n","    if min_value is None:\n","        min_value = divisor\n","    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n","    # Make sure that round down does not go down by more than 10%.\n","    if new_v < 0.9 * v:\n","        new_v += divisor\n","    return new_v\n","\n","\n","def hard_sigmoid(x, inplace: bool = False):\n","    if inplace:\n","        return x.add_(3.).clamp_(0., 6.).div_(6.)\n","    else:\n","        return F.relu6(x + 3.) / 6.\n","\n","#Going to change this to CBAM, which is not published by the author and included spatial connections as well as depth connections in SE\n","\n","class SqueezeExcite(nn.Module):\n","    def __init__(self, in_chs, se_ratio=0.25, reduced_base_chs=None,\n","                 act_layer=nn.ReLU, gate_fn=hard_sigmoid, divisor=4, **_):\n","        super(SqueezeExcite, self).__init__()\n","        self.gate_fn = gate_fn\n","        reduced_chs = _make_divisible((reduced_base_chs or in_chs) * se_ratio, divisor)\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.conv_reduce = nn.Conv2d(in_chs, reduced_chs, 1, bias=True)\n","        self.act1 = act_layer(inplace=True)\n","        self.conv_expand = nn.Conv2d(reduced_chs, in_chs, 1, bias=True)\n","\n","    def forward(self, x):\n","        x_se = self.avg_pool(x)\n","        x_se = self.conv_reduce(x_se)\n","        x_se = self.act1(x_se)\n","        x_se = self.conv_expand(x_se)\n","        x = x * self.gate_fn(x_se)\n","        return x\n","\n","\n","class ConvBnAct(nn.Module):\n","    def __init__(self, in_chs, out_chs, kernel_size,\n","                 stride=1, act_layer=nn.ReLU):\n","        super(ConvBnAct, self).__init__()\n","        self.conv = nn.Conv2d(in_chs, out_chs, kernel_size, stride, kernel_size//2, bias=False)\n","        self.bn1 = nn.BatchNorm2d(out_chs)\n","        self.act1 = act_layer(inplace=True)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.bn1(x)\n","        x = self.act1(x)\n","        return x\n","\n","\n","class GhostModule(nn.Module):\n","    def __init__(self, inp, oup, kernel_size=1, ratio=2, dw_size=3, stride=1, relu=True):\n","        super(GhostModule, self).__init__()\n","        self.oup = oup\n","        init_channels = math.ceil(oup / ratio)\n","        new_channels = init_channels*(ratio-1)\n","\n","        self.primary_conv = nn.Sequential(\n","            nn.Conv2d(inp, init_channels, kernel_size, stride, kernel_size//2, bias=False),\n","            nn.BatchNorm2d(init_channels),\n","            nn.ReLU(inplace=True) if relu else nn.Sequential(),\n","        )\n","\n","        self.cheap_operation = nn.Sequential(\n","            nn.Conv2d(init_channels, new_channels, dw_size, 1, dw_size//2, groups=init_channels, bias=False),\n","            nn.BatchNorm2d(new_channels),\n","            nn.ReLU(inplace=True) if relu else nn.Sequential(),\n","        )\n","\n","    def forward(self, x):\n","        x1 = self.primary_conv(x)\n","        x2 = self.cheap_operation(x1)\n","        out = torch.cat([x1,x2], dim=1)\n","        return out[:,:self.oup,:,:]\n","\n","\n","class GhostBottleneck(nn.Module):\n","    \"\"\" Ghost bottleneck w/ optional SE\"\"\" #I will be using CBAM\n","\n","    def __init__(self, in_chs, mid_chs, out_chs, dw_kernel_size=3,\n","                 stride=1, act_layer=nn.ReLU, se_ratio=0.):\n","        super(GhostBottleneck, self).__init__()\n","        has_se = se_ratio is not None and se_ratio > 0.\n","        self.stride = stride\n","\n","        # Point-wise expansion\n","        self.ghost1 = GhostModule(in_chs, mid_chs, relu=True)\n","\n","        # Depth-wise convolution\n","        if self.stride > 1:\n","            self.conv_dw = nn.Conv2d(mid_chs, mid_chs, dw_kernel_size, stride=stride,\n","                             padding=(dw_kernel_size-1)//2,\n","                             groups=mid_chs, bias=False)\n","            self.bn_dw = nn.BatchNorm2d(mid_chs)\n","\n","        # Squeeze-and-excitation\n","        if has_se:\n","            self.se = SqueezeExcite(mid_chs, se_ratio=se_ratio)\n","        else:\n","            self.se = None\n","\n","        # Point-wise linear projection\n","        self.ghost2 = GhostModule(mid_chs, out_chs, relu=False)\n","\n","        # shortcut\n","        if (in_chs == out_chs and self.stride == 1):\n","            self.shortcut = nn.Sequential()\n","        else:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_chs, in_chs, dw_kernel_size, stride=stride,\n","                       padding=(dw_kernel_size-1)//2, groups=in_chs, bias=False),\n","                nn.BatchNorm2d(in_chs),\n","                nn.Conv2d(in_chs, out_chs, 1, stride=1, padding=0, bias=False),\n","                nn.BatchNorm2d(out_chs),\n","            )\n","\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        # 1st ghost bottleneck\n","        x = self.ghost1(x)\n","\n","        # Depth-wise convolution\n","        if self.stride > 1:\n","            x = self.conv_dw(x)\n","            x = self.bn_dw(x)\n","\n","        # Squeeze-and-excitation\n","        if self.se is not None:\n","            x = self.se(x)\n","\n","        # 2nd ghost bottleneck\n","        x = self.ghost2(x)\n","\n","        x += self.shortcut(residual)\n","        return x\n","\n","\n","class GhostNet(nn.Module):\n","    def __init__(self, cfgs, num_classes=1000, width=1.0, dropout=0.2):\n","        super(GhostNet, self).__init__()\n","        # setting of inverted residual blocks\n","        self.cfgs = cfgs\n","        self.dropout = dropout\n","\n","        # building first layer\n","        output_channel = _make_divisible(16 * width, 4)\n","        self.conv_stem = nn.Conv2d(3, output_channel, 3, 2, 1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(output_channel)\n","        self.act1 = nn.ReLU(inplace=True)\n","        input_channel = output_channel\n","\n","        # building inverted residual blocks\n","        stages = []\n","        block = GhostBottleneck\n","        for cfg in self.cfgs:\n","            layers = []\n","            for k, exp_size, c, se_ratio, s in cfg:\n","                output_channel = _make_divisible(c * width, 4)\n","                hidden_channel = _make_divisible(exp_size * width, 4)\n","                layers.append(block(input_channel, hidden_channel, output_channel, k, s,\n","                              se_ratio=se_ratio))\n","                input_channel = output_channel\n","            stages.append(nn.Sequential(*layers))\n","\n","        output_channel = _make_divisible(exp_size * width, 4)\n","        stages.append(nn.Sequential(ConvBnAct(input_channel, output_channel, 1)))\n","        input_channel = output_channel\n","\n","        self.blocks = nn.Sequential(*stages)\n","\n","        # building last several layers\n","        output_channel = 1280\n","        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.conv_head = nn.Conv2d(input_channel, output_channel, 1, 1, 0, bias=True)\n","        self.act2 = nn.ReLU(inplace=True)\n","        self.classifier = nn.Linear(output_channel, num_classes)\n","\n","    def forward(self, x):\n","        x = self.conv_stem(x)\n","        x = self.bn1(x)\n","        x = self.act1(x)\n","        x = self.blocks(x)\n","        x = self.global_pool(x)\n","        x = self.conv_head(x)\n","        x = self.act2(x)\n","        x = x.view(x.size(0), -1)\n","        if self.dropout > 0.:\n","            x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.classifier(x)\n","        return x\n","\n","\n","def ghostnet(**kwargs):\n","    \"\"\"\n","    Constructs a GhostNet model\n","    \"\"\"\n","    cfgs = [\n","        # k, t, c, SE, s\n","        # stage1\n","        [[3,  16,  16, 0, 1]],\n","        # stage2\n","        [[3,  48,  24, 0, 2]],\n","        [[3,  72,  24, 0, 1]],\n","        # stage3\n","        [[5,  72,  40, 0.25, 2]],\n","        [[5, 120,  40, 0.25, 1]],\n","        # stage4\n","        [[3, 240,  80, 0, 2]],\n","        [[3, 200,  80, 0, 1],\n","         [3, 184,  80, 0, 1],\n","         [3, 184,  80, 0, 1],\n","         [3, 480, 112, 0.25, 1],\n","         [3, 672, 112, 0.25, 1]\n","        ],\n","        # stage5\n","        [[5, 672, 160, 0.25, 2]],\n","        [[5, 960, 160, 0, 1],\n","         [5, 960, 160, 0.25, 1],\n","         [5, 960, 160, 0, 1],\n","         [5, 960, 160, 0.25, 1]\n","        ]\n","    ]\n","    return GhostNet(cfgs, **kwargs)\n","\n","\n","if __name__=='__main__':\n","    model = ghostnet()\n","    model.eval()\n","    print(model)\n","    input = torch.randn(32,3,320,256)\n","    y = model(input)\n","    print(y.size())"]}]}