{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"162zSWaRukF1pekWhc6kA9z9kx2Fb1wJE","timestamp":1741314573263}],"authorship_tag":"ABX9TyPWmaK/yL8PW0X8VXi6zENK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"EBJD7t3Xcy1P"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","\n","\n","__all__ = ['ghostnetN1']\n","\n","\n","def _make_divisible(v, divisor, min_value=None):\n","    \"\"\"\n","    This function is taken from the original tf repo.\n","    It ensures that all layers have a channel number that is divisible by 8\n","    It can be seen here:\n","    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n","    \"\"\"\n","    if min_value is None:\n","        min_value = divisor\n","    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n","    # Make sure that round down does not go down by more than 10%.\n","    if new_v < 0.9 * v:\n","        new_v += divisor\n","    return new_v\n","\n","\n","#This function was introduced in the author's code. It is an approximation of the sigmoid function,\n","#removing the complexities of the exponential component, thus making its application more lightweight.\n","#I have chosen to keep this function as I would like to keep my model lightweight but include spacial attention:\n","def hard_sigmoid(x, inplace: bool = False):\n","    if inplace:\n","        return x.add_(3.).clamp_(0., 6.).div_(6.)\n","    else:\n","        return F.relu6(x + 3.) / 6.\n","\n","#Rather than using the Author's implementation of Squeeze and Excite, which only uses channel attention, I have decided to reimplement this\n","#program using Convolutional Block Attention Model (CBAM) which adds spacial attention as well as channel attention:\n","#Important Changes Made:\n","#1)I have chosen to include spacial attention, so I include a kernel attribute which I will pass through in GhostNet configuration:\n","#2)I have chosen to keep the channel attribute ratio the same in order to keep the computational cost down as\n","      # I have already added a spacial component increasing some complexity to this mini network design.\n","class CBAM(nn.Module):\n","    def __init__(self, input_channels, cbam_ratio=0.25, reduced_base_chs=None, spatial_kernel = 7,\n","                 act_layer=nn.ReLU, gate_fn=hard_sigmoid, divisor=4, **_):  #By default, the ratio is active and kernel is 7. This will allow for both attention types\n","\n","        super(CBAM, self).__init__()\n","        # Activation function for attention, using Hard_sigmoid to reduce complexity.\n","        self.gate_fn = gate_fn\n","\n","        # Compute reduced channels for Channel Attention:\n","        reduced_channels = _make_divisible((reduced_base_chs or input_channels) * cbam_ratio, divisor)\n","\n","        # Channel Attention Module (CAM):\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.max_pool = nn.AdaptiveMaxPool2d(1)\n","        self.conv_reduce = nn.Conv2d(input_channels, reduced_channels, 1, bias=True)\n","        self.act1 = act_layer(inplace=True)\n","        self.conv_expand = nn.Conv2d(reduced_channels, input_channels, 1, bias=True)\n","\n","        # Spatial Attention Module (SAM):\n","        self.conv_spatial = nn.Conv2d(2, 1, kernel_size=spatial_kernel, padding=spatial_kernel // 2, bias=False)\n","\n","    def forward(self, x):\n","        # Channel Attention\n","        # Note: This is very similiar to SE, so the new component will be (SAM)\n","        x_avg = self.avg_pool(x)\n","        x_max = self.max_pool(x)  #I added max_pool to the SE portion for this model\n","        x_ca = self.conv_reduce(x_avg) + self.conv_reduce(x_max)  # Shared MLP\n","        x_ca = self.act1(x_ca)\n","        x_ca = self.conv_expand(x_ca)\n","        x = x * self.gate_fn(x_ca)  # Appling the Channel Attention to the model\n","\n","        # Spatial Attention\n","        x_avg_sp = torch.mean(x, dim=1, keepdim=True)  # Avg Pool along channel axis\n","        x_max_sp, _ = torch.max(x, dim=1, keepdim=True)  # Max Pool along channel axis\n","        x_sa = torch.cat([x_avg_sp, x_max_sp], dim=1)  # Concatenate along channel dim\n","        x_sa = self.conv_spatial(x_sa)\n","        x = x * self.gate_fn(x_sa)  # Appling the Spatial Attention to the model\n","        return x\n","\n","\n","#Class to combine a normal convolutional layer with batch normalization:\n","class ConvBnAct(nn.Module):\n","    def __init__(self, in_chs, out_chs, kernel_size,\n","                 stride=1, act_layer=nn.ReLU):\n","        super(ConvBnAct, self).__init__()\n","        self.conv = nn.Conv2d(in_chs, out_chs, kernel_size, stride, kernel_size//2, bias=False)\n","        self.bn1 = nn.BatchNorm2d(out_chs)\n","        self.act1 = act_layer(inplace=True)\n","\n","    #Perform forward pass:\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.bn1(x)\n","        x = self.act1(x)\n","        return x\n","\n","\n","class GhostModule(nn.Module):\n","    def __init__(self, inp, oup, kernel_size=1, ratio=2, dw_size=3, stride=1, relu=True): #NOTE: The ratio will remain 2 for my model, but I will test how changing it will effect the model later\n","        super(GhostModule, self).__init__()\n","        self.oup = oup\n","        init_channels = math.ceil(oup / ratio)  #This will be the number of real channels.\n","        new_channels = init_channels*(ratio-1)  #This is the number of linearly transformed channels.\n","\n","        #EX: if you had 16 out channels with a ratio of 2, there would be 8 normal convolution channels and 8 channels created from cheap operations\n","        #Note if ratio was increased to 8, then there would be 2 normal convolution channels and 14 cheap operations.\n","\n","        #Perform primary convolution:\n","        self.primary_conv = nn.Sequential(\n","            nn.Conv2d(inp, init_channels, kernel_size, stride, kernel_size//2, bias=False),\n","            nn.BatchNorm2d(init_channels),\n","            nn.ReLU(inplace=True) if relu else nn.Sequential(),\n","        )\n","\n","        #Perform cheap linear transformation on remaining channels. This is dependent on the ratio.\n","        self.cheap_operation = nn.Sequential(\n","            nn.Conv2d(init_channels, new_channels, dw_size, 1, dw_size//2, groups=init_channels, bias=False),\n","            nn.BatchNorm2d(new_channels),\n","            nn.ReLU(inplace=True) if relu else nn.Sequential(),\n","        )\n","\n","    #Forward Pass:\n","    def forward(self, x):\n","        x1 = self.primary_conv(x)\n","        x2 = self.cheap_operation(x1)\n","        out = torch.cat([x1,x2], dim=1)\n","        return out[:,:self.oup,:,:]\n","\n","\n","class GhostBottleneck(nn.Module):\n","    \"\"\" Ghost bottleneck with CBAM Implementation\"\"\"\n","\n","    def __init__(self, input_channels, mid_channels, out_channels, dw_kernel_size=3,\n","                 stride=1, act_layer=nn.ReLU, cbam_ratio=0., spatial_kernel = 7):\n","\n","        super(GhostBottleneck, self).__init__()\n","        has_cbam = cbam_ratio is not None and cbam_ratio > 0.\n","        self.stride = stride\n","\n","        # Point-wise expansion\n","        self.ghost1 = GhostModule(input_channels, mid_channels, relu=True)\n","\n","        # Depth-wise convolution (Ignored unless stride is called on to be greater than 1)\n","        if self.stride > 1:\n","            self.conv_dw = nn.Conv2d(mid_channels, mid_channels, dw_kernel_size, stride=stride,\n","                             padding=(dw_kernel_size-1)//2,\n","                             groups=mid_channels, bias=False)\n","            self.bn_dw = nn.BatchNorm2d(mid_channels)\n","\n","        # Convolutional Block Attention Model (CBAM) Activation:\n","        #For initial implementation, I plan to have minimal spacial attention, and then test a second model with greater spacial attention\n","        if has_cbam:\n","            self.cbam = CBAM(mid_channels, cbam_ratio=cbam_ratio, spatial_kernel = spatial_kernel)\n","        else:\n","            self.cbam = None\n","\n","        # Point-wise linear projection:\n","        self.ghost2 = GhostModule(mid_channels, out_channels, relu=False)\n","\n","        # Shortcut:\n","        if (input_channels == out_channels and self.stride == 1):\n","            self.shortcut = nn.Sequential()\n","        else:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(input_channels, input_channels, dw_kernel_size, stride=stride, padding=(dw_kernel_size-1)//2, groups=input_channels, bias=False),\n","                nn.BatchNorm2d(input_channels),\n","                nn.Conv2d(input_channels, out_channels, 1, stride=1, padding=0, bias=False),\n","                nn.BatchNorm2d(out_channels),\n","            )\n","\n","    #Forward Pass:\n","    def forward(self, x):\n","        residual = x\n","\n","        # 1st ghost bottleneck:\n","        x = self.ghost1(x)\n","\n","        # Depth-wise convolution:\n","        if self.stride > 1:\n","            x = self.conv_dw(x)\n","            x = self.bn_dw(x)\n","\n","        # CBAM:\n","        if self.cbam is not None:\n","            x = self.cbam(x)\n","\n","        # 2nd ghost bottleneck:\n","        x = self.ghost2(x)\n","\n","        x += self.shortcut(residual)\n","        return x\n","\n","\n","class GhostNet_N(nn.Module):\n","    def __init__(self, cfgs, num_classes=1000, width=1.0, dropout=0.2):\n","\n","        super(GhostNet_N, self).__init__()\n","        # setting of inverted residual blocks\n","        self.cfgs = cfgs    #Model will use the configuration defined below: *I have added an additional tear to allocate spacial kernel size.\n","        self.dropout = dropout\n","\n","        #Building First Layers:\n","        output_channel = _make_divisible(16 * width, 4)\n","        self.conv_stem = nn.Conv2d(3, output_channel, 3, 2, 1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(output_channel)\n","        self.act1 = nn.ReLU(inplace=True)\n","        input_channel = output_channel\n","\n","        # Building inverted residual blocks:\n","        #I have added a spacial_kernel component to the architecture to input different sized spacial attention filters during the network.\n","        stages = []\n","        block = GhostBottleneck\n","        for cfg in self.cfgs:\n","            layers = []\n","            for k, exp_size, c, cbam_ratio, s, spatial_kernel in cfg:\n","                output_channel = _make_divisible(c * width, 4)\n","                hidden_channel = _make_divisible(exp_size * width, 4)\n","                layers.append(block(input_channel, hidden_channel, output_channel, k, s, cbam_ratio=cbam_ratio, spatial_kernel = spatial_kernel)) #This will create a layer when spacial kernel is active. I only choose to use spacial attention when channel attential is used.\n","                input_channel = output_channel\n","            stages.append(nn.Sequential(*layers))\n","\n","        output_channel = _make_divisible(exp_size * width, 4)\n","        stages.append(nn.Sequential(ConvBnAct(input_channel, output_channel, 1)))\n","        input_channel = output_channel\n","\n","        self.blocks = nn.Sequential(*stages)\n","\n","        # Building last layers:\n","        output_channel = 1280\n","        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.conv_head = nn.Conv2d(input_channel, output_channel, 1, 1, 0, bias=True)\n","        self.act2 = nn.ReLU(inplace=True)\n","        self.classifier = nn.Linear(output_channel, num_classes)\n","\n","    #Define forward pass:\n","    def forward(self, x):\n","        x = self.conv_stem(x)\n","        x = self.bn1(x)\n","        x = self.act1(x)\n","        x = self.blocks(x)\n","        x = self.global_pool(x)\n","        x = self.conv_head(x)\n","        x = self.act2(x)\n","        x = x.view(x.size(0), -1)\n","        if self.dropout > 0.:\n","            x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.classifier(x)\n","        return x\n","\n","\n","def ghostnetN1(**kwargs):\n","    \"\"\"\n","    Constructs a GhostNet model with CBAM\n","    \"\"\"\n","    cfgs = [\n","        # k = kernel size, t = expansion factor , c = # of Feature Maps, CBAM = cbam ratio, s = stride, sk = spatial kernel\n","        # stage1\n","        [[3,  16,  16, 0, 1,0]],  #The Spatial Kernel Size is set to 0 when CBAM is not active for a particular layer of the GhostNet\n","        # stage2\n","        [[3,  48,  24, 0, 2,0]],\n","        [[3,  72,  24, 0, 1,0]],\n","        # stage3\n","        [[5,  72,  40, 0.25, 2,7]], #Keep lower spacial attention for intitial layers to a kernel size of 7, looking at more global spacial attention\n","        [[5, 120,  40, 0.25, 1,7]],\n","        # stage4\n","        [[3, 240,  80, 0, 2,0]],\n","        [[3, 200,  80, 0, 1,0],\n","         [3, 184,  80, 0, 1,0],\n","         [3, 184,  80, 0, 1,0],\n","         [3, 480, 112, 0.25, 1,5],  #Spacial Attention at the end layers to 5. This is starting to focus the attention\n","         [3, 672, 112, 0.25, 1,5]\n","        ],\n","        # stage5\n","        [[5, 672, 160, 0.25, 2, 3]], #The final layer will get the most extensive spacial attention with a kernel size of 3. The spacial attention is very focused.\n","        [[5, 960, 160, 0, 1,0],\n","         [5, 960, 160, 0.25, 1,3],\n","         [5, 960, 160, 0, 1,0],\n","         [5, 960, 160, 0.25, 1,3]\n","        ]\n","    ]\n","    return GhostNet_N(cfgs, **kwargs)\n","\n","\n","if __name__=='__main__':\n","    model = ghostnetN1()\n","    model.eval()\n","    print(model)\n","    input = torch.randn(32,3,320,256)\n","    y = model(input)\n","    print(y.size())"]}]}