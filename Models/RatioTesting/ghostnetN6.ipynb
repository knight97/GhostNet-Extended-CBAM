{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Ik2p1b5iey8z-yE-OYnYkaY-1VB3s3oH","timestamp":1743976385826},{"file_id":"1S0QNWWmmC2u66tuO21SSylBTBQUazmaH","timestamp":1743954056200},{"file_id":"1wFPBIHI-0rZjxkExu64sYXzlSaTLO5HD","timestamp":1743830013176},{"file_id":"162zSWaRukF1pekWhc6kA9z9kx2Fb1wJE","timestamp":1741314573263}],"authorship_tag":"ABX9TyMJT/jHWKvdnDWU8Jf2UMKe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"EBJD7t3Xcy1P"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","\n","\n","__all__ = ['ghostnetN6']\n","\n","\n","def _make_divisible(v, divisor, min_value=None):\n","    \"\"\"\n","    This function is taken from the original tf repo.\n","    It ensures that all layers have a channel number that is divisible by 8\n","    It can be seen here:\n","    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n","    \"\"\"\n","    if min_value is None:\n","        min_value = divisor\n","    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n","    # Make sure that round down does not go down by more than 10%.\n","    if new_v < 0.9 * v:\n","        new_v += divisor\n","    return new_v\n","\n","\n","#This function was introduced in the author's code. It is an approximation of the sigmoid function,\n","#removing the complexities of the exponential component, thus making its application more lightweight.\n","def hard_sigmoid(x, inplace: bool = False):\n","    if inplace:\n","        return x.add_(3.).clamp_(0., 6.).div_(6.)\n","    else:\n","        return F.relu6(x + 3.) / 6.\n","\n","#Rather than using the Author's implementation of Squeeze and Excite, which only uses channel attention, I have decided to reimplement this\n","#program using Convolutional Block Attention Model (CBAM) which adds spacial attention as well:\n","#Important Changes Made:\n","#1)I have chosen to include spacial attention, so I include a kernel attribute which I will pass through in GhostNet\n","#2)I have chosen to keep the channel attribute ratio the same in order to keep the computational cost down as I have already added a spacial component.\n","class CBAM(nn.Module):\n","    def __init__(self, input_channels, cbam_ratio=0.25, reduced_base_chs=None, spatial_kernel = 7,\n","                 act_layer=nn.ReLU, gate_fn=hard_sigmoid, divisor=4, **_):\n","\n","        super(CBAM, self).__init__()\n","        self.gate_fn = gate_fn  # Activation function for attention, using Hard_sigmoid to reduce complexity.\n","\n","        # Compute reduced channels for Channel Attention\n","        reduced_channels = _make_divisible((reduced_base_chs or input_channels) * cbam_ratio, divisor)\n","\n","        # Channel Attention Module (CAM)\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.max_pool = nn.AdaptiveMaxPool2d(1)\n","        self.conv_reduce = nn.Conv2d(input_channels, reduced_channels, 1, bias=True)\n","        self.act1 = act_layer(inplace=True)\n","        self.conv_expand = nn.Conv2d(reduced_channels, input_channels, 1, bias=True)\n","\n","        # Spatial Attention Module (SAM)\n","        self.conv_spatial = nn.Conv2d(2, 1, kernel_size=spatial_kernel, padding=spatial_kernel // 2, bias=False)\n","\n","    def forward(self, x):\n","        # Channel Attention\n","        x_avg = self.avg_pool(x)\n","        x_max = self.max_pool(x)\n","        x_ca = self.conv_reduce(x_avg) + self.conv_reduce(x_max)  # Shared MLP\n","        x_ca = self.act1(x_ca)\n","        x_ca = self.conv_expand(x_ca)\n","        x = x * self.gate_fn(x_ca)  # Appling the Channel Attention\n","\n","        # Spatial Attention\n","        x_avg_sp = torch.mean(x, dim=1, keepdim=True)  # Avg Pool along channel axis\n","        x_max_sp, _ = torch.max(x, dim=1, keepdim=True)  # Max Pool along channel axis\n","        x_sa = torch.cat([x_avg_sp, x_max_sp], dim=1)  # Concatenate along channel dim\n","        x_sa = self.conv_spatial(x_sa)\n","        x = x * self.gate_fn(x_sa)  # Appling the Spatial Attention\n","        return x\n","\n","\n","class ConvBnAct(nn.Module):\n","    def __init__(self, in_chs, out_chs, kernel_size,\n","                 stride=1, act_layer=nn.ReLU):\n","        super(ConvBnAct, self).__init__()\n","        self.conv = nn.Conv2d(in_chs, out_chs, kernel_size, stride, kernel_size//2, bias=False)\n","        self.bn1 = nn.BatchNorm2d(out_chs)\n","        self.act1 = act_layer(inplace=True)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.bn1(x)\n","        x = self.act1(x)\n","        return x\n","\n","\n","class GhostModule(nn.Module):\n","    def __init__(self, inp, oup, kernel_size=1, ratio=6, dw_size=3, stride=1, relu=True):\n","        super(GhostModule, self).__init__()\n","        self.oup = oup\n","        init_channels = math.ceil(oup / ratio)\n","        new_channels = init_channels*(ratio-1)\n","\n","        self.primary_conv = nn.Sequential(\n","            nn.Conv2d(inp, init_channels, kernel_size, stride, kernel_size//2, bias=False),\n","            nn.BatchNorm2d(init_channels),\n","            nn.ReLU(inplace=True) if relu else nn.Sequential(),\n","        )\n","\n","        self.cheap_operation = nn.Sequential(\n","            nn.Conv2d(init_channels, new_channels, dw_size, 1, dw_size//2, groups=init_channels, bias=False),\n","            nn.BatchNorm2d(new_channels),\n","            nn.ReLU(inplace=True) if relu else nn.Sequential(),\n","        )\n","\n","        #I want to add additional code to the ghost module to merge the primary convolutions with the first layer of ghost operations:\n","\n","    def forward(self, x):\n","        x1 = self.primary_conv(x)\n","        x2 = self.cheap_operation(x1)\n","        out = torch.cat([x1,x2], dim=1)\n","        return out[:,:self.oup,:,:]\n","\n","\n","class GhostBottleneck(nn.Module):\n","    \"\"\" Ghost bottleneck with CBAM Implementation\"\"\"\n","\n","    def __init__(self, input_channels, mid_channels, out_channels, dw_kernel_size=3,\n","                 stride=1, act_layer=nn.ReLU, cbam_ratio=0., spatial_kernel = 7):\n","\n","        super(GhostBottleneck, self).__init__()\n","        has_cbam = cbam_ratio is not None and cbam_ratio > 0.\n","        self.stride = stride\n","\n","        # Point-wise expansion\n","        self.ghost1 = GhostModule(input_channels, mid_channels, relu=True)\n","\n","        # Depth-wise convolution (Ignored unless stride is called on to be greater)\n","        if self.stride > 1:\n","            self.conv_dw = nn.Conv2d(mid_channels, mid_channels, dw_kernel_size, stride=stride,\n","                             padding=(dw_kernel_size-1)//2,\n","                             groups=mid_channels, bias=False)\n","            self.bn_dw = nn.BatchNorm2d(mid_channels)\n","\n","        # Convolutional Block Attention Model (CBAM) Activation:\n","        #For initial implementation, I would like to test the spacial kernel size to be 7\n","        if has_cbam:\n","            self.cbam = CBAM(mid_channels, cbam_ratio=cbam_ratio, spatial_kernel = spatial_kernel)\n","        else:\n","            self.cbam = None\n","\n","        # Point-wise linear projection\n","        self.ghost2 = GhostModule(mid_channels, out_channels, relu=False)\n","\n","        # shortcut\n","        if (input_channels == out_channels and self.stride == 1):\n","            self.shortcut = nn.Sequential()\n","        else:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(input_channels, input_channels, dw_kernel_size, stride=stride, padding=(dw_kernel_size-1)//2, groups=input_channels, bias=False),\n","                nn.BatchNorm2d(input_channels),\n","                nn.Conv2d(input_channels, out_channels, 1, stride=1, padding=0, bias=False),\n","                nn.BatchNorm2d(out_channels),\n","            )\n","\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        # 1st ghost bottleneck\n","        x = self.ghost1(x)\n","\n","        # Depth-wise convolution\n","        if self.stride > 1:\n","            x = self.conv_dw(x)\n","            x = self.bn_dw(x)\n","\n","        # CBAM\n","        if self.cbam is not None:\n","            x = self.cbam(x)\n","\n","        # 2nd ghost bottleneck\n","        x = self.ghost2(x)\n","\n","        x += self.shortcut(residual)\n","        return x\n","\n","\n","class GhostNet_N6(nn.Module):\n","    def __init__(self, cfgs, num_classes=1000, width=1.0, dropout=0.2):\n","\n","        super(GhostNet_N6, self).__init__()\n","        # setting of inverted residual blocks\n","        self.cfgs = cfgs\n","        self.dropout = dropout\n","\n","        #Building First Layer\n","        output_channel = _make_divisible(16 * width, 4)\n","        self.conv_stem = nn.Conv2d(3, output_channel, 3, 2, 1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(output_channel)\n","        self.act1 = nn.ReLU(inplace=True)\n","        input_channel = output_channel\n","\n","        # building inverted residual blocks\n","        #I have added a spacial_kernel component to the architecture to input different sized spacial attention filters during the network.\n","        stages = []\n","        block = GhostBottleneck\n","        for cfg in self.cfgs:\n","            layers = []\n","            for k, exp_size, c, cbam_ratio, s, spatial_kernel in cfg:\n","                output_channel = _make_divisible(c * width, 4)\n","                hidden_channel = _make_divisible(exp_size * width, 4)\n","                layers.append(block(input_channel, hidden_channel, output_channel, k, s, cbam_ratio=cbam_ratio, spatial_kernel = spatial_kernel))\n","                input_channel = output_channel\n","            stages.append(nn.Sequential(*layers))\n","\n","        output_channel = _make_divisible(exp_size * width, 4)\n","        stages.append(nn.Sequential(ConvBnAct(input_channel, output_channel, 1)))\n","        input_channel = output_channel\n","\n","        self.blocks = nn.Sequential(*stages)\n","\n","        # building last several layers\n","        output_channel = 1280\n","        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.conv_head = nn.Conv2d(input_channel, output_channel, 1, 1, 0, bias=True)\n","        self.act2 = nn.ReLU(inplace=True)\n","        self.classifier = nn.Linear(output_channel, num_classes)\n","\n","    def forward(self, x):\n","        x = self.conv_stem(x)\n","        x = self.bn1(x)\n","        x = self.act1(x)\n","        x = self.blocks(x)\n","        x = self.global_pool(x)\n","        x = self.conv_head(x)\n","        x = self.act2(x)\n","        x = x.view(x.size(0), -1)\n","        if self.dropout > 0.:\n","            x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.classifier(x)\n","        return x\n","\n","\n","def ghostnetN6(**kwargs):\n","    \"\"\"\n","    Constructs a GhostNet model\n","    \"\"\"\n","    cfgs = [\n","        # k = kernel size, t = expansion factor , c = # of Feature Maps, CBAM = cbam ratio, s = stride, sk = spatial kernel\n","        # stage1\n","        [[3,  16,  16, 0, 1,0]],  #The Spatial Kernel Size is set to 0 when CBAM is not active for a particular layer of the GhostNet\n","        # stage2\n","        [[3,  48,  24, 0, 2,0]],\n","        [[3,  72,  24, 0, 1,0]],\n","        # stage3\n","        [[5,  72,  40, 0.25, 2,7]], #Keep the default spacial attention for intitial layers to a kernel size of 7\n","        [[5, 120,  40, 0.25, 1,7]],\n","        # stage4\n","        [[3, 240,  80, 0, 2,0]],\n","        [[3, 200,  80, 0, 1,0],\n","         [3, 184,  80, 0, 1,0],\n","         [3, 184,  80, 0, 1,0],\n","         [3, 480, 112, 0.25, 1,5],  #Increase the Spacial Attention at the end layers to 5\n","         [3, 672, 112, 0.25, 1,5]\n","        ],\n","        # stage5\n","        [[5, 672, 160, 0.25, 2, 3]], #The final layer will get the most extensive spacial attention with a kernel size of 3. (This can be changed if complexity is increased too much)\n","        [[5, 960, 160, 0, 1,0],\n","         [5, 960, 160, 0.25, 1,3],\n","         [5, 960, 160, 0, 1,0],\n","         [5, 960, 160, 0.25, 1,3]\n","        ]\n","    ]\n","    return GhostNet_N8(cfgs, **kwargs)\n","\n","\n","if __name__=='__main__':\n","    model = ghostnetN6()\n","    model.eval()\n","    print(model)\n","    input = torch.randn(32,3,320,256)\n","    y = model(input)\n","    print(y.size())"]}]}
